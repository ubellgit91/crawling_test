# 디씨 인사이드 개념글 크롤링

## 기본 크롤링 구조

```python
from bs4 import BeautifulSoup
from urllib import request

html = request.urlopen('http://gall.dcinside.com/board/lists?id=comic_new1&exception_mode=recommend')
soup = BeautifulSoup(html, 'lxml')

# 기준
link = soup.find_all("td",{"class": "gall_tit ub-word"})

for m in link:
    if m.find("em",{"class":"icon_survey"}):
        pass
    elif m.find("em",{"class":"icon_notice"}):
        pass
    else:
        print(m.a.text)
```

기본적인 개념글 제목 크롤링 소스이다.

urllib 패키지의 request.py 모듈을 import해온다.

그리고 request.urlopen()함수로 해당 url에 request를 보내서 open시키고 html 정보를 response 시킨다.

그 후 **html을 parsing하는 BeutifulSoup를 이용**해서 html을 lxml 구조로 파씽시킨다.

BeautifulSoup(html, '파씽하고자 하는 구조나 사용할 parser')

그 후 **lxml 구조로 파씽된** html문서를 find_all()를 이용해서 내부구조를 훑어 내가 찾고자 하는 조건에 맞는 것들을 찾는다.

find_All(tag, attributes, recursive, text, limit, keywords)

찾고자 하는 tag이름, 그 태그들에서 찾고자하는 attribute<태그에서의 속성값들>를 지정한다. 보통 이 두개만으로도 찾는다. 

이것들은 크롬 개발자도구(F12)를 열어서 확인할 수 있다.

`link = soup.find_all("td",{"class": "gall_tit ub-word"})`

이 부분을 보면, td태그 안에(td태그에 포함된 하위태그들도 포함함) class 태그 속성이 gall_tit ub_word 라는 속성을 갖는 애들을 찾는다는 뜻이다.



> http://blog.naver.com/PostView.nhn?blogId=93immm&logNo=220914062738&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView 참조함
>
> https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 
>
> bs4 공식문서. BS는 html을 parsing하는 것!@!!!



## 짤방이 있는 글만 가져오기

만약 찾은 위치가 필요한 경우 find()를 사용하고 그 여부만 확인하려면 in을 사용하는 것이 일반적이다.

find()는 일치하는 문자열을 찾으면 그 문자열의 인덱스번호를 반환한다.

in은 찾고자 하는 문자열이 해당 문자열에 존재하는지 여부만 판단한다. (boolean)

>mysite = 'Site name is webisfree.'
>if 'webisfree' in myname:
> print('Okay')
>
>myname이라는 문자열 변수 안에 webisfree라는 문자열이 존재하므로 Okay를 출력함.

find()의 경우,

>위에서 시작점과 끝나는 점을 지정하는 2, 3번째 인자값은 옵션사항입니다. 그럼 아래 예제를 봐주세요.
>
>myStr = "abcdefghijklmn"
>search = "e"
>
>indexNo = myStr.find(search)
>print indexNo
>
>이제 위의 코드의 실행하면 다음과 같이 해당하는 위치값을 나타나게됩니다. 시작점부터 0을 기준으로 합니다.
>
>4
>
>만약 찾는 값이 없는 경우 어떻게 될까요? 이 경우 숫자 -1을 반환합니다.
>
>출처 https://webisfree.com/2017-08-23/python%EC%97%90%EC%84%9C-match-string-%EC%9D%BC%EC%B9%98%ED%95%98%EB%8A%94-%EB%AC%B8%EC%9E%90%EC%97%B4-%EC%B0%BE%EB%8A%94-%EB%B0%A9%EB%B2%95

라고 한다.

-------------------------------

개념글 중에서 '번역'이 들어간 번역글만 크롤링 하는게 목적이므로

```python
from bs4 import BeautifulSoup
from urllib import request
#
import requests


html = request.urlopen('http://gall.dcinside.com/board/lists?id=comic_new1&exception_mode=recommend')
soup = BeautifulSoup(html, 'lxml')

link = soup.find_all('td', {'class': 'gall_tit ub-word'})

for m in link:
    if m.find("em", {"class": "icon_survey"}):
        pass
    elif m.find("em", {"class": "icon_notice"}):
        pass
    elif m.find("em", {"class": "icon_issue"}):
        pass
    else:
        if '번역' in m.a.text:
            print(m.a.text)
```

이런 식으로 일단 a태그의 텍스트에 "번역"이 들어간 글만 가져온다.

` else:
​        if '번역' in m.a.text:
​            print(m.a.text)`

m에는 link에서 하나하나 뽑아온 각 td태그가 들어있고, td태그 안에는 a태그가 있는 구조다. td > a > em 이런 구조였음.

그러니까 m(td)의 child-tag인 a로 접근하고, a태그의 text값을 가져온다.

그 text값들 중에 '번역'이라는 텍스트가 들어있는 글만 활용할 것이다.

여기까지만 해도 사실상 번역글 링크는 가져올 수 있음.

## 짤방이 있는 글에 들어가서 자동으로 이미지들 다운받기

짤방이 있고 , 제목에 "번역"이라는 글이 들어간 글만 긁어 와 봤다. 이후 해당 글에 들어가서 이미지를 다운받아 보자.

```python
if '번역' in m.a.text:
	print(m.a.text) # 제목 출력.
    added_link = fixed_link + m.a.get('href')
    html = request.urlopen(added_link) # 해당 게시글로 href 주소를 이용해서 접속 request.
    soup = BeautifulSoup(html, 'lxml')
    img_ul = soup.find_all("ul", {"class": "appending_file"}) # 게시글에서 업로드된 파일목록을 담고있는 ul태그를 찾음. 결과는 resultset타입으로 나옴.
    if len(img_ul) > 0: # 첨부이미지들 목록을 표시하는 ul태그가 있으면 1, 없으면 0. 즉 0이면 첨부파일이 없음.
    img_li = img_ul[0].find_all('li')
    print(img_li)
    print(type(img_li))
    for n in img_li:
    	file_name = n.a.text
    	print('filename', file_name) # li태그의 하위태그 a의 text를 가져옴.
    	file_link = n.a.get('href') # a태그의 href속성 값을 가져온다.
    	# href속성의 url주소값으로 request를 보내서 이미지를 다운로드 한다.
    	tmp_a = "http://image.dcinside.com/download.php"
    	tmp_b = "http://dcimg6.dcinside.co.kr/viewimage.php"
    	file_link2 = file_link.replace(tmp_a, tmp_b)
    	mk_req = request.Request(file_link, headers=hdr)
    	# mk_req.add_header(hdr)# url주소와 url주소로 보낼 header를 갖는 request를 만든다.
    	print(mk_req)
    	img_file = request.urlopen(mk_req) #
    	full_dir = m.a.text+"_"+n.a.text
    	# print(img_file)
    	# wb는 The wb indicates that the file is opened for writing in binary mode. jpg 파일같은 거 구성할 땐 binary mode로 써야함. 읽는건 rb
    	# 순수 텍스트는 w,
    	with open(os.path.join(BASE_DIR, full_dir), 'wb') as f:
   		f.write(img_file.read())
```

> BeautifulSoup으로 요소를 검색하면 검색결과는 resultSet type으로 나온다.

m.a 으로 td안의 a태그에 접근할 수 있다. 이 점을 이용해서 a태그의 href속성을 가져오도록 한다.

`m.a.get('href')` get()함수를 이용해서 a태그 안의 href 속성 값을 가져온다. 

그 값을 기존 url에 + 해서 변수에 담는다. 그렇게 해당 게시글에 접속하는 url을 완성시킨다. 

`html = request.urlopen(added_link)` 그리고 request.urlopen()을 이용해서 url값을 보내 url을 연다.

urlopen()으로 url을 열면, html 이 response되어 담기는거 같다.

html을 BS를 이용해서 파이썬이 읽을 수 있도록 lxml 타입으로 파씽한다. 

게시글 내부에 보면, 업로드된 이미지 파일들의 목록을 담고 있는 태그가 있다. ul태그 중에 appending_file이라는 class속성을 가진 ul 안에 li태그 목록으로 가지고 있었다.

```python
img_ul = soup.find_all("ul", {"class": "appending_file"}) # 게시글에서 업로드된 파일목록을 담고있는 ul태그를 찾음. 결과는 resultset타입으로 나옴.
if len(img_ul) > 0: # 첨부이미지들 목록을 표시하는 ul태그가 있으면 1, 없으면 0. 즉 0이면 첨부파일이 없음.
    img_li = img_ul[0].find_all('li')
```

결과는 resultset타입으로 나오고, 게시글 당 ul태그에 class="appending_file"을 갖는 값은 하나뿐이라서 img_ul에는 값이 하나만 담긴다.  리스트 타입으로 담기는지 , 0번 인덱스에 담아져 있었다.

img_ul[0]안에 있는 li태그들을 모두 찾아서 담는다.

**img_li**에는 이제 ul태그 안에 있는 li태그들이 모두 담겨있다고 보면 된다.

----------------------------



## 이미지 다운로드 해보기

```python
    for n in img_li:
    	file_name = n.a.text
    	print('filename', file_name) # li태그의 하위태그 a의 text를 가져옴.
    	file_link = n.a.get('href') # a태그의 href속성 값을 가져온다.
    	# href속성의 url주소값으로 request를 보내서 이미지를 다운로드 한다.
    	tmp_a = "http://image.dcinside.com/download.php"
    	tmp_b = "http://dcimg6.dcinside.co.kr/viewimage.php"
    	file_link2 = file_link.replace(tmp_a, tmp_b)
    	mk_req = request.Request(file_link, headers=hdr)
    	# mk_req.add_header(hdr)# url주소와 url주소로 보낼 header를 갖는 request를 만든다.
    	print(mk_req)
    	img_file = request.urlopen(mk_req) #
    	full_dir = m.a.text+"_"+n.a.text
    	# print(img_file)
    	# wb는 The wb indicates that the file is opened for writing in binary mode. jpg 파일같은 거 구성할 땐 binary mode로 써야함. 읽는건 rb
    	# 순수 텍스트는 w,
    	with open(os.path.join(BASE_DIR, full_dir), 'wb') as f:
   		f.write(img_file.read())
```

이미지를 다운로드 하는 부분이다.

li태그 안에 있는 a태그의 text로 업로드된 파일들을 확인한다.

a태그 안에 이미지들이 업로드된 경로가 href 속성으로 담겨있다. 

`n.a.get('href')`로 이미지의 경로를 알아낸다. 근데 DC인사이드는 해당 이미지 다운로드 경로가 그대로 있는 것이 아니라 다운로드 시 , 중간에 경로가 바뀐다고...

내가 참조한 사이트에서는 선행되는 주소 `tmp_a` 부분을 `tmp_b`로 변경해서 쓰도록 했다. 저 url주소로 하면 다운로드가 된다고...

`file_link2 = file_link.replace(tmp_a, tmp_b)` replace(기존, 변경)를 이용해서 기존값에 일치하는 값을 변경하고자 하는 값으로 변경한다.

그 값을 

`mk_req = request.Request(file_link, headers=hdr)`

request.Request()를 이용해서 request객체로 만든다. request()는 요청할 request를 객체형태로 만들어주는 클래스이다.

```
hdr = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Connection': 'keep-alive',
    'Host': 'image.dcinside.com',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.80 Safari/537.36',
}
```

왜 urlopen()으로 바로 url을 보내는게 아니라 request객체를 만들어서 만든 request객체를 urlopen()으로 보내는 방식을 썼냐면, headers에 request header를 넣기 위해서다.

디씨에서 크롤링을 막기위해서 header값을 조회하도록 바뀌어서 header 값이 필요하다고...

> Request에 관한 정보(header값, request method, 처리결과 등등), Response에 관한 정보등은
>
> 크롬 개발자도구(F12)를 이용하면 간단히 볼 수 있다. 개발자도구를 열고 Network 탭으로 이동하면 request나 response가 발생할 때마다 Network탭에 쌓인다.
>
> request 목록에 있는 것들을 눌러보면 해당 request가 보내질 때의 Header값 등을 확인할 수 있다.

아무튼, header값을 넣은 request를 urlopen()으로 request를 전송한다. 

`img_file = request.urlopen(mk_req)`

제대로 보내졌으면 response로 이미지 파일이 들어와 있어야한다. 해당 response는 img_file안에 들어있다.

이걸 파일I/O하는 기능을 이용해서 이미지 파일을 만든다. 

```python
with open(os.path.join(BASE_DIR, full_dir), 'wb') as f:
   		f.write(img_file.read())
```

with 구문을 이용하면 open과 close가 자동으로 처리된다고 한다. 

open으로 파일을 쓰기위한 공간을 열고, 해당 공간이 어떤 작용을 할지 두번째 인자값으로 지정해준다. 

wb는 writing in binary mode 라고해서 이미지값을 파일에 writing할 때 쓴다고 한다.

w나 r은 text파일을 읽고 쓸 때 쓴다. 

열려있는 공간을 as를 이용해 이름을 f로 지정해 준다.

그 후, img_file을 read()한 값을 f.write()를 이용해서 열려있는 파일공간에다가 write해준다.



이 과정이 모두 끝나면 지정한 경로에 이미지파일이 생긴다.

근데 이미지 다운로드 경로가 잘못된건지 0byte파일들만 생기고 이미지가 정상적으로 만들어지지 않는다. -_- 내가 잘못한 부분이 분명 있겠지.

---------------------------------



## HTTP header

1. HTTP 헤더 및 바디(본체,본문)

  ㅇ 헤더
​     - 웹 서버 및 클라이언트 사이에서, 
​        . 일반 문서 데이터(바디 본문) 이외에, 추가적인 정보를 교환할 수 있도록,
​        . HTTP 메세지 선두에 삽입되는 요소로 수십개의 다양한 종류를 갖음

  ㅇ 바디 (본체,본문)
​     - 실제 내용이 들어있는 부분

  ※ 헤더 및 바디 간의 구분은 빈 줄(CRLF)에 의함
​     - 바디는 일반 텍스트 형태 이외에도 이진 데이터 형식(이미지 등)도 가능


2. 일반적인 헤더 구성   ☞ HTTP 메세지 참조


  ※ 위에서, Blank Line(빈줄,CRLF)는 HTTP 헤더 항목들과 본문 시작을 구분시켜 줌


3. HTTP 헤더 항목 종류

   ㅇ 요청 헤더 (Request Header) 항목
      - 요청 헤더는 요청 메세지 내에서만 나타나며 가장 방대함

      - 주요 항목들                                              ☞ HTTP 요청 헤더 항목 참조
         . Host, From, Cookie, Referer, User-Agent, Accept, If-Modified-Since 등


   ㅇ 응답 헤더 (Response Header) 항목
​      - 특정 유형의 HTTP 요청이나 특정 HTTP 헤더를 수신했을때, 이에 응답 함

      - 주요 항목들                                              ☞ HTTP 응답 헤더 항목 참조
         . Server, Set-Cookie, Accept-Range, Age, ETag, Proxy-authenticate 등


   ㅇ 일반 헤더 (General Header) 항목
​      - 일반 목적의(기본적인) 헤더 항목
​         . 요청 및 응답 메세지 모두에서 사용 가능

      - 주요 항목들                                              ☞ HTTP 일반 헤더 항목 참조
         . Date, Connection, Cache-Control, Pragma, Trailer 등


  ㅇ 엔터티/개체 헤더 (Entity Header) 항목
​     - 선택적인 개체(콘텐츠,본문,리소스 등) 그 자체를 설명함
​        . 요청 및 응답 메세지 모두에서 사용 가능

     * HTTP 메세지는 컨테이너이고, 엔터티는 컨테이너가 실어나르는 개별 화물들 임
    
     - 주요 항목들                                             ☞ HTTP 엔터티 헤더 항목 참조
        . Content-Type, Content-Language, Content-Encoding, Content-Length, Content-Location,
          Location, Allow, Expires, Last-Modified, Transfer-Encoding 등
